{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Final Combined Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wdir = \"/home/abyun/Downloads/w251_proj/completed_csv/\"\n",
    "\n",
    "#Ensure the current directory is correctly set\n",
    "os.chdir(wdir)\n",
    "#Build a Pandas data frame to hold the new data\n",
    "col_names_topic = [\"Topic\",\"Date\",\"Author\",\"Author_numposts\",\"Author_type\",\"Author_karma\",\"Response_date\",\n",
    "                   \"Responder\",\"Responder_numposts\",\"Responder_type\",\"Responder_karma\"]\n",
    "col_names_profile = [\"User\",\"User_numposts\",\"User_type\",\"User_karma\",\"Date_registered\",\"Last_active\"]\n",
    "df_topic_combined = pd.DataFrame(columns = col_names_topic)\n",
    "df_profile_combined = pd.DataFrame(columns = col_names_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in os.listdir(os.getcwd()): #Loops through all files in the current directory\n",
    "    if i.find(\"df_profile\") != -1: #forum profile page\n",
    "        df_1 = pd.read_csv(i,header=0,names=col_names_profile)       \n",
    "        df_profile_combined = df_profile_combined.append(df_1)    \n",
    "            \n",
    "    elif i.find(\"df_topic\") != -1: #forum topic page\n",
    "        df_2 = pd.read_csv(i,header=0,names=col_names_topic)\n",
    "        df_topic_combined = df_topic_combined.append(df_2)    \n",
    "            \n",
    "#Remove duplicates and reindex\n",
    "df_profile_combined = df_profile_combined.drop_duplicates(keep=\"first\")\n",
    "df_profile_combined.index = range(df_profile_combined.shape[0])\n",
    "\n",
    "df_topic_combined = df_topic_combined.drop_duplicates(keep=\"first\")\n",
    "df_topic_combined.index = range(df_topic_combined.shape[0])\n",
    "\n",
    "df_profile_combined.to_csv(\"df_profile_SR1_combined.csv\", encoding=\"utf=8\")\n",
    "df_topic_combined.to_csv(\"df_topic_SR1_combined.csv\",encoding=\"utf-8\")\n",
    "        \n",
    "os.chdir(wdir) #Reset current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from dateutil.parser import parse\n",
    "\n",
    "#Build a Pandas data frame to hold the new data\n",
    "col_names_topic = [\"Topic\",\"Date\",\"Author\",\"Author_numposts\",\"Author_type\",\"Author_karma\",\"Response_date\",\n",
    "                   \"Responder\",\"Responder_numposts\",\"Responder_type\",\"Responder_karma\"]\n",
    "col_names_profile = [\"User\",\"User_numposts\",\"User_type\",\"User_karma\",\"Date_registered\",\"Last_active\"]\n",
    "df_topic_combined = pd.DataFrame(columns = col_names_topic)\n",
    "df_profile_combined = pd.DataFrame(columns = col_names_profile)\n",
    "\n",
    "for i in os.listdir(os.getcwd()): #Loops through all files in the current directory\n",
    "    print i\n",
    "    if i.find(\"df_profile\") != -1: #forum profile page\n",
    "        df_1 = pd.read_csv(i,header=0,names=col_names_profile)       \n",
    "        df_profile_combined = df_profile_combined.append(df_1)    \n",
    "            \n",
    "    elif i.find(\"df_topic\") != -1: #forum topic page\n",
    "        df_2 = pd.read_csv(i,header=0,names=col_names_topic)\n",
    "        df_topic_combined = df_topic_combined.append(df_2)    \n",
    "            \n",
    "#Remove duplicates and reindex\n",
    "df_profile_combined = df_profile_combined.drop_duplicates(keep=\"first\")\n",
    "df_profile_combined.index = range(df_profile_combined.shape[0])\n",
    "\n",
    "df_topic_combined = df_topic_combined.drop_duplicates(keep=\"first\")\n",
    "df_topic_combined.index = range(df_topic_combined.shape[0])\n",
    "\n",
    "df_profile_combined.to_csv(\"df_profile_SR1_combined.csv\", encoding=\"utf=8\")\n",
    "df_topic_combined.to_csv(\"df_topic_SR1_combined.csv\",encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Read in Final CSV and Load into Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wdir = \"/home/abyun/Downloads/w251_proj/completed_csv/\"\n",
    "\n",
    "#Ensure the current directory is correctly set\n",
    "os.chdir(wdir)\n",
    "#Build a Pandas data frame to hold the new data\n",
    "col_names_topic = [\"Topic\",\"Date\",\"Author\",\"Author_numposts\",\"Author_type\",\"Author_karma\",\"Response_date\",\n",
    "                   \"Responder\",\"Responder_numposts\",\"Responder_type\",\"Responder_karma\"]\n",
    "col_names_profile = [\"User\",\"User_numposts\",\"User_type\",\"User_karma\",\"Date_registered\",\"Last_active\"]\n",
    "df_topic_combined = pd.DataFrame(columns = col_names_topic)\n",
    "df_profile_combined = pd.DataFrame(columns = col_names_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "profile_file_name = 'df_profile_SR1_combined.csv'\n",
    "topic_file_name = 'df_topic_SR1_combined.csv'\n",
    "\n",
    "df_profile_combined = pd.read_csv(profile_file_name,header=0,names=col_names_profile)         \n",
    "df_topic_combined = pd.read_csv(topic_file_name,header=0,names=col_names_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34427\n",
      "        User        User_numposts   User_type User_karma      Date_registered  \\\n",
      "0  Zringo333  53 (13.250 per day)  Jr. Member      +0/-0  2013-10-06 02:58:00   \n",
      "\n",
      "           Last_active  \n",
      "0  2013-10-10 07:41:00  \n",
      "391329\n",
      "                                          Topic                 Date  \\\n",
      "0                          Prescription meds...  2011-06-20 04:18:00   \n",
      "1      Why cant/wont the mods stop the spammers  2013-03-23 14:42:00   \n",
      "2      Why cant/wont the mods stop the spammers  2013-03-23 14:42:00   \n",
      "3      Why cant/wont the mods stop the spammers  2013-03-23 14:42:00   \n",
      "4      Why cant/wont the mods stop the spammers  2013-03-23 14:42:00   \n",
      "5      Why cant/wont the mods stop the spammers  2013-03-23 14:42:00   \n",
      "6      Why cant/wont the mods stop the spammers  2013-03-23 14:42:00   \n",
      "7      Why cant/wont the mods stop the spammers  2013-03-23 14:42:00   \n",
      "8  Re: Why cant/wont the mods stop the spammers  2013-03-23 20:18:00   \n",
      "9  Re: Why cant/wont the mods stop the spammers  2013-03-23 20:18:00   \n",
      "\n",
      "     Author  Author_numposts Author_type Author_karma        Response_date  \\\n",
      "0  brownbud               40      Newbie        +0/-0                 0:00   \n",
      "1  sofish89              641         NaN      +89/-60  2016-03-08 00:00:00   \n",
      "2  sofish89              641         NaN      +89/-60  2016-03-08 00:00:00   \n",
      "3  sofish89              641         NaN      +89/-60  2016-03-08 00:00:00   \n",
      "4  sofish89              641         NaN      +89/-60  2016-03-08 00:00:00   \n",
      "5  sofish89              641         NaN      +89/-60  2016-03-08 00:00:00   \n",
      "6  sofish89              641         NaN      +89/-60  2016-03-08 00:00:00   \n",
      "7  sofish89              641         NaN      +89/-60  2016-03-08 00:00:00   \n",
      "8   ChemCat             3550         NaN    +355/-146  2016-03-08 00:00:00   \n",
      "9   ChemCat             3550         NaN    +355/-146  2016-03-08 00:00:00   \n",
      "\n",
      "         Responder  Responder_numposts Responder_type Responder_karma  \n",
      "0              NaN                 NaN            NaN             NaN  \n",
      "1          ChemCat                3550            NaN       +355/-146  \n",
      "2  jailbirdslanger                 167            NaN           +6/-6  \n",
      "3         ladyjane                 207            NaN         +82/-18  \n",
      "4        nanpa2001                 718            NaN         +88/-57  \n",
      "5          Duckman                 225            NaN          +10/-6  \n",
      "6         flaxceed                 561            NaN         +34/-30  \n",
      "7            scout                1593            NaN       +425/-327  \n",
      "8            astor                3114            NaN       +640/-252  \n",
      "9            scout                1593            NaN       +425/-327  \n"
     ]
    }
   ],
   "source": [
    "print len(df_profile_combined)\n",
    "print df_profile_combined.head(1)\n",
    "print len(df_topic_combined)\n",
    "print df_topic_combined.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USE dnm;\n",
    "CREATE ColumnFamily profiles(\n",
    "User_Date text,\n",
    "Username text,\n",
    "User_numposts text,\n",
    "User_type text,\n",
    "User_karma text,\n",
    "Date_registered text,\n",
    "Source text,\n",
    "PRIMARY KEY (User_Date)) WITH COMPACT STORAGE;\n",
    "\n",
    "USE dnm;\n",
    "CREATE ColumnFamily topics(\n",
    "Author_Date_Responder_Date text,\n",
    "Topic text,\n",
    "Date text,\n",
    "Author text,\n",
    "Author_numposts text,\n",
    "Author_type text,\n",
    "Author_karma text,\n",
    "Response_date text,\n",
    "Responder text,\n",
    "Responder_numposts text,\n",
    "Responder_type text,\n",
    "Responder_karma text,\n",
    "Source text,\n",
    "PRIMARY KEY (Author_Date_Responder_Date)) WITH COMPACT STORAGE;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Script to Load to Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from dateutil.parser import parse\n",
    "import hashlib\n",
    "import sys  \n",
    "reload(sys)  \n",
    "sys.setdefaultencoding('utf8')\n",
    "\n",
    "from pycassa.pool import ConnectionPool\n",
    "from pycassa.columnfamily import ColumnFamily\n",
    "\n",
    "def df_to_cassandra(df, db_type, SR_type):\n",
    "    c = 0\n",
    "    if db_type == 'profile':\n",
    "        cf = ColumnFamily(pool, 'profiles')\n",
    "        for index, row in df.iterrows():\n",
    "            try:\n",
    "                Date_registered = str(parse(row[\"Date_registered\"]))\n",
    "                Username = str(row[\"User\"])\n",
    "                Username = Username.encode('ascii', 'ignore')\n",
    "                User_numposts = str(row[\"User_numposts\"])\n",
    "                User_type = str(row[\"User_type\"])\n",
    "                User_karma = str(row[\"User_karma\"])\n",
    "                Source = SR_type\n",
    "                User_Date = hashlib.md5((Username + Date_registered).encode()).hexdigest()\n",
    "\n",
    "                row = {\"user_date\":User_Date,\"username\":Username,\"user_numposts\":User_numposts,\n",
    "                       \"user_type\":User_type,\"user_karma\":User_karma,\"date_registered\":Date_registered,\n",
    "                      \"source\":Source}\n",
    "                key = row[\"user_date\"]\n",
    "                cf.insert(key,row)\n",
    "            except:\n",
    "                print \"error, skip row\"\n",
    "            c+=1\n",
    "            if c%1000==0: print c \n",
    "\n",
    "    elif db_type == 'topic': \n",
    "        cf = ColumnFamily(pool, 'topics')\n",
    "        for index, row in df.iterrows():\n",
    "            try:\n",
    "                Date = str(parse(row[\"Date\"]))\n",
    "                Topic = str(row[\"Topic\"])\n",
    "                Topic = Topic.encode('ascii', 'ignore')\n",
    "                Author = str(row[\"Author\"])\n",
    "                Author = Author.encode('ascii', 'ignore')\n",
    "                Author_numposts = str(row[\"Author_numposts\"])\n",
    "                Author_type = str(row[\"Author_type\"])\n",
    "                Author_karma = str(row[\"Author_karma\"])\n",
    "                Response_date = str(parse(row[\"Response_date\"]))\n",
    "                Responder = str(row[\"Responder\"])\n",
    "                Responder = Responder.encode('ascii', 'ignore')\n",
    "                Responder_numposts = str(row[\"Responder_numposts\"])\n",
    "                Responder_type = str(row[\"Responder_type\"])\n",
    "                Responder_karma = str(row[\"Responder_karma\"])\n",
    "                Source = SR_type\n",
    "                Author_Date_Responder_Date = hashlib.md5((Author+Date+Responder+Response_date)\n",
    "                                                         .encode()).hexdigest()\n",
    "\n",
    "                row = {\"author_date_responder_date\":Author_Date_Responder_Date,\"topic\":Topic,\"date\":Date,\n",
    "                       \"author\":Author,\"author_numposts\":Author_numposts,\"author_type\":Author_type,\n",
    "                       \"author_karma\":Author_karma,\"response_date\":Response_date,\"responder\":Responder,\n",
    "                       \"responder_numposts\":Responder_numposts,\"responder_type\":Responder_type,\n",
    "                       \"responder_karma\":Responder_karma,\"source\":Source}\n",
    "                key = row[\"author_date_responder_date\"]\n",
    "                cf.insert(key,row)\n",
    "            except:\n",
    "                print \"error, skip row\"\n",
    "            c+=1\n",
    "            if c%1000==0: print c \n",
    "\n",
    "pool = ConnectionPool('dnm', ['158.85.217.74:9160']) \n",
    "\n",
    "col_names_topic = [\"Topic\",\"Date\",\"Author\",\"Author_numposts\",\"Author_type\",\"Author_karma\",\"Response_date\",\n",
    "                   \"Responder\",\"Responder_numposts\",\"Responder_type\",\"Responder_karma\"]\n",
    "col_names_profile = [\"User\",\"User_numposts\",\"User_type\",\"User_karma\",\"Date_registered\",\"Last_active\"]\n",
    "df_topic_combined = pd.DataFrame(columns = col_names_topic)\n",
    "df_profile_combined = pd.DataFrame(columns = col_names_profile)\n",
    "df_topic_combined2 = pd.DataFrame(columns = col_names_topic)\n",
    "df_profile_combined2 = pd.DataFrame(columns = col_names_profile)\n",
    "\n",
    "profile_file_name = 'df_profile_SR1_combined.csv'\n",
    "topic_file_name = 'df_topic_SR1_combined.csv'\n",
    "\n",
    "df_profile_combined = pd.read_csv(profile_file_name,header=0,names=col_names_profile)         \n",
    "df_topic_combined = pd.read_csv(topic_file_name,header=0,names=col_names_topic)\n",
    "\n",
    "profile_file_name2 = 'df_profile_SR2_combined.csv'\n",
    "topic_file_name2 = 'df_topic_SR2_combined.csv'\n",
    "\n",
    "df_profile_combined2 = pd.read_csv(profile_file_name2,header=0,names=col_names_profile)         \n",
    "df_topic_combined2 = pd.read_csv(topic_file_name2,header=0,names=col_names_topic)\n",
    "\n",
    "print \"starting\"\n",
    "df_to_cassandra(df_profile_combined, \"profile\", \"Silk_Road_1\")\n",
    "print \"SR1 profile complete\"\n",
    "df_to_cassandra(df_topic_combined, \"topic\", \"Silk_Road_1\")\n",
    "print \"SR1 topic complete\"\n",
    "\n",
    "#df_to_cassandra(df_profile_combined2, \"profile\", \"Silk_Road_2\")\n",
    "#print \"SR2 profile complete\"\n",
    "#df_to_cassandra(df_topic_combined2, \"topic\", \"Silk_Road_2\")\n",
    "#print \"SR2 topic complete\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
